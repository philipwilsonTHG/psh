#!/usr/bin/env python3
"""Token type definitions for PSH lexer and parser."""

from dataclasses import dataclass, field
from enum import Enum, auto
from typing import TYPE_CHECKING, List, Optional

if TYPE_CHECKING:
    from .lexer.token_parts import TokenPart


class TokenType(Enum):
    """All token types recognized by the shell lexer."""
    # Basic tokens
    WORD = auto()
    PIPE = auto()
    REDIRECT_IN = auto()
    REDIRECT_OUT = auto()
    REDIRECT_APPEND = auto()
    REDIRECT_DUP = auto()
    HEREDOC = auto()
    HEREDOC_STRIP = auto()
    HERE_STRING = auto()
    SEMICOLON = auto()
    AMPERSAND = auto()
    AND_AND = auto()
    OR_OR = auto()
    NEWLINE = auto()
    EOF = auto()

    # Quoted strings and variables
    STRING = auto()
    VARIABLE = auto()

    # Expansions
    COMMAND_SUB = auto()
    COMMAND_SUB_BACKTICK = auto()
    ARITH_EXPANSION = auto()
    PARAM_EXPANSION = auto()       # ${var:-default} style expansions
    PROCESS_SUB_IN = auto()    # <(...)
    PROCESS_SUB_OUT = auto()   # >(...)

    # Grouping
    LPAREN = auto()
    RPAREN = auto()
    LBRACE = auto()
    RBRACE = auto()
    LBRACKET = auto()
    RBRACKET = auto()
    DOUBLE_LPAREN = auto()  # ((
    DOUBLE_RPAREN = auto()  # ))

    # Keywords
    FUNCTION = auto()
    IF = auto()
    THEN = auto()
    ELSE = auto()
    FI = auto()
    ELIF = auto()
    WHILE = auto()
    UNTIL = auto()
    DO = auto()
    DONE = auto()
    FOR = auto()
    IN = auto()
    BREAK = auto()
    CONTINUE = auto()
    RETURN = auto()
    CASE = auto()
    ESAC = auto()
    SELECT = auto()

    # Case terminators
    DOUBLE_SEMICOLON = auto()  # ;;
    SEMICOLON_AMP = auto()     # ;&
    AMP_SEMICOLON = auto()     # ;;&

    # Special operators
    EXCLAMATION = auto()       # !
    DOUBLE_LBRACKET = auto()   # [[
    DOUBLE_RBRACKET = auto()   # ]]
    REGEX_MATCH = auto()       # =~
    EQUAL = auto()             # ==
    NOT_EQUAL = auto()         # !=

    # Assignment operators
    ASSIGN = auto()            # =
    PLUS_ASSIGN = auto()       # +=
    MINUS_ASSIGN = auto()      # -=
    MULT_ASSIGN = auto()       # *=
    DIV_ASSIGN = auto()        # /=
    MOD_ASSIGN = auto()        # %=
    AND_ASSIGN = auto()        # &=
    OR_ASSIGN = auto()         # |=
    XOR_ASSIGN = auto()        # ^=
    LSHIFT_ASSIGN = auto()     # <<=
    RSHIFT_ASSIGN = auto()     # >>=

    # Pattern matching
    GLOB_STAR = auto()         # * in patterns
    GLOB_QUESTION = auto()     # ? in patterns
    GLOB_BRACKET = auto()      # [...] in patterns

    # Context-specific operators
    LESS_THAN_TEST = auto()    # < in test context
    GREATER_THAN_TEST = auto() # > in test context
    LESS_EQUAL_TEST = auto()   # <= in test context
    GREATER_EQUAL_TEST = auto() # >= in test context

    # Special constructs
    HERE_DELIMITER = auto()    # Heredoc delimiter
    ASSIGNMENT_WORD = auto()   # VAR=value pattern
    ARRAY_ASSIGNMENT_WORD = auto() # arr[index]=value pattern

    # Composite tokens
    COMPOSITE = auto()         # Merged adjacent tokens


@dataclass
class Token:
    """Unified token class for the shell lexer and parser."""
    type: TokenType
    value: str
    position: int
    end_position: int = 0  # Position after the last character of the token
    quote_type: Optional[str] = None  # Track the quote character used (' or " or None)
    line: Optional[int] = None  # Line number (1-based)
    column: Optional[int] = None  # Column number (1-based)
    adjacent_to_previous: bool = False  # True if no whitespace between this and previous token
    is_keyword: bool = False  # True when keyword normalizer marks this as a keyword
    parts: Optional[List['TokenPart']] = field(default=None)  # Token parts (imported from lexer.token_parts)

    def __post_init__(self):
        """Initialize parts if not provided."""
        if self.parts is None:
            self.parts = []

    @classmethod
    def from_basic_token(
        cls,
        type: TokenType,
        value: str,
        position: int,
        end_position: int = 0,
        quote_type: Optional[str] = None,
        line: Optional[int] = None,
        column: Optional[int] = None,
        adjacent_to_previous: bool = False
    ) -> 'Token':
        """Create unified Token from basic token information."""
        return cls(
            type=type,
            value=value,
            position=position,
            end_position=end_position,
            quote_type=quote_type,
            adjacent_to_previous=adjacent_to_previous,
            line=line,
            column=column
        )

    @property
    def normalized_value(self) -> str:
        """Return a canonical representation suitable for comparisons."""
        value = self.value or ""

        # Prefer canonical keyword spelling when available
        try:
            from .lexer.keyword_defs import keyword_from_type  # Late import avoids cycles
        except ImportError:
            keyword_from_type = None  # type: ignore
        if keyword_from_type is not None:
            canonical = keyword_from_type(self.type)
            if canonical:
                return canonical

        # If marked as a keyword, normalize to lowercase
        if self.is_keyword:
            return value.lower()

        return value
