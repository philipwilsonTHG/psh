# Parser Subsystem

This document provides guidance for working with the PSH parser subsystem.

## Architecture Overview

The parser transforms token streams into Abstract Syntax Trees (ASTs). PSH uses a **recursive descent parser** with specialized sub-parsers for different language constructs.

```
Tokens → Parser → AST (CommandList/TopLevel)
              ↓
    ┌─────────┼─────────┬──────────┬─────────┐
    ↓         ↓         ↓          ↓         ↓
Statements Commands  Control   Functions  Tests
                    Structures
```

## Key Files

### Core Parser (`recursive_descent/`)

| File | Purpose |
|------|---------|
| `parser.py` | Main `Parser` class - orchestrates all parsing |
| `context.py` | `ParserContext` - centralized state management |
| `base_context.py` | `ContextBaseParser` - base class with context integration |
| `helpers.py` | `TokenGroups`, `ErrorContext`, `ParseError` |

### Specialized Parsers (`recursive_descent/parsers/`)

| File | Parses |
|------|--------|
| `statements.py` | Statement lists, command lists, pipelines |
| `commands.py` | Simple commands, arguments, redirections |
| `control_structures.py` | `if`, `while`, `for`, `case`, `select` |
| `tests.py` | `[[ ]]` test expressions |
| `arithmetic.py` | `(( ))` arithmetic expressions |
| `functions.py` | Function definitions |
| `redirections.py` | I/O redirections, heredocs |
| `arrays.py` | Array assignments |

### Support Infrastructure (`recursive_descent/support/`)

| File | Purpose |
|------|---------|
| `context_factory.py` | Factory functions for creating configured contexts |
| `word_builder.py` | Build Word AST nodes from tokens |
| `utils.py` | Parser utilities |

### Parser Combinators (`combinators/`) -- Experimental

**Status: Experimental / Educational.** This is NOT the production parser.
It exists as an educational counterpoint demonstrating functional parsing,
and as a proof of concept that parser combinators can handle real shell
syntax. There is no plan to converge with or replace the recursive descent
parser. It may lag behind on edge-case fixes and new features.

See [Combinator Parser Guide](../../docs/guides/combinator_parser_guide.md)
for a detailed walkthrough. Use `parser-select combinator` inside psh to
try it interactively.

| File | Purpose |
|------|---------|
| `core.py` | Combinator primitives (`token`, `many`, `sequence`, etc.) |
| `tokens.py` | Token-level matchers |
| `expansions.py` | Expansion parsers and Word AST building |
| `commands.py` | Simple commands, pipelines, and-or lists |
| `control_structures.py` | if, while, for, case, select, functions, groups |
| `special_commands.py` | `(( ))`, `[[ ]]`, arrays, process substitution |
| `heredoc_processor.py` | Post-parse heredoc content population |
| `parser.py` | `ParserCombinatorShellParser` integration class |

### Validation (`validation/`)

| File | Purpose |
|------|---------|
| `validation_pipeline.py` | AST validation after parsing |
| `semantic_analyzer.py` | Semantic analysis |
| `validation_rules.py` | Specific validation rules |

## Core Patterns

### 1. Delegating Parser Pattern

The main `Parser` delegates to specialized sub-parsers:

```python
class Parser(ContextBaseParser):
    def __init__(self, ...):
        # Sub-parsers for different constructs
        self.statements = StatementParser(self)
        self.commands = CommandParser(self)
        self.control_structures = ControlStructureParser(self)
        self.tests = TestParser(self)
        self.arithmetic = ArithmeticParser(self)
        self.functions = FunctionParser(self)
        self.redirections = RedirectionParser(self)
        self.arrays = ArrayParser(self)
```

### 2. Sub-Parser Contract

All 8 sub-parsers follow the same implicit contract:

- **Initialization**: `__init__(self, main_parser)` stores `self.parser`
  (the main `Parser` instance). There is no shared base class enforcing
  this -- it is a convention.
- **State access**: Use `self.parser.peek()`, `.advance()`, `.match()`,
  `.expect()`, `.consume_if()`, etc. (methods inherited from
  `ContextBaseParser`).
- **Token position**: Use `self.parser.current` (the property on
  `Parser`), not `self.parser.ctx.current` directly. Both work, but the
  property is the intended public interface.
- **Context manager**: Use `with self.parser.ctx:` **only** when the
  method needs to change a parsing state flag. The context manager
  saves all flags on entry and restores them on exit. Sub-parsers that
  don't change flags correctly omit it.

  | Sub-parser | Method | Flag changed |
  |---|---|---|
  | ArithmeticParser | `_parse_arithmetic_neutral` | `in_arithmetic` |
  | TestParser | `parse_enhanced_test_statement` | `in_test_expr` |
  | ControlStructureParser | `parse_case_item` | `in_case_pattern` |
  | FunctionParser | `parse_compound_command` | `in_function_body` |

  CommandParser, StatementParser, RedirectionParser, and ArrayParser
  never change flags and never use the context manager.

- **Optional consumption**: Prefer `self.parser.consume_if(TokenType.X)`
  over the inline `if self.parser.match(X): self.parser.advance()`
  pattern.
- **Error creation**: Use `self.parser.error(message)` or
  `self.parser.error(message, token)`.

### 3. ParserContext State Management

`ParserContext` tracks all parsing state:

```python
class ParserContext:
    tokens: List[Token]      # Token stream
    current: int             # Current position

    # Parsing state flags
    in_function_body: bool
    in_arithmetic: bool
    in_test_expr: bool
    in_case_pattern: bool
    in_command_substitution: bool

    # Scope tracking
    scope_stack: List[str]   # ["function", "loop", ...]

    # Error handling
    config: ParserConfig
    errors: List[ParseError]
```

### 4. Context Manager for State Preservation

Use `with self.parser.ctx:` to save/restore parsing state flags (see
the sub-parser contract above for when this is appropriate):

```python
# In control_structures.py
def parse_case_item(self):
    with self.parser.ctx:  # Saves state
        self.parser.ctx.in_case_pattern = True
        pattern_str = self._parse_case_pattern()
    # in_case_pattern automatically restored
```

### 5. TokenGroups for Matching

Predefined token sets for common checks:

```python
class TokenGroups:
    WORD_LIKE = frozenset({WORD, STRING, VARIABLE, ...})
    REDIRECTS = frozenset({REDIRECT_IN, REDIRECT_OUT, ...})
    STATEMENT_SEPARATORS = frozenset({SEMICOLON, NEWLINE, ...})
    CONTROL_KEYWORDS = frozenset({IF, WHILE, FOR, ...})
```

## Parsing Flow

### Top-Level Parsing

```python
def parse(self) -> TopLevel:
    top_level = TopLevel()
    while not self.at_end():
        item = self._parse_top_level_item()  # Function def or statement
        top_level.items.append(item)
    return self._simplify_result(top_level)
```

### Statement Parsing

```
Statement → AndOrList (&&/|| chains)
AndOrList → Pipeline (| chains)
Pipeline → Command (simple or compound)
Command → SimpleCommand | IfConditional | WhileLoop | ...
```

## Common Tasks

### Adding a New Control Structure

1. Add token types in `psh/token_types.py`

2. Add AST node in `psh/ast_nodes.py`:
```python
@dataclass
class MyNewStructure(Command):
    condition: Command
    body: List[Statement]
```

3. Add parser method in `parsers/control_structures.py`:
```python
def parse_my_structure(self) -> MyNewStructure:
    self.parser.expect(TokenType.MY_KEYWORD)
    condition = self.parser.commands.parse_command()
    self.parser.expect(TokenType.DO)
    body = self.parser.statements.parse_command_list_until(TokenType.DONE)
    self.parser.expect(TokenType.DONE)
    return MyNewStructure(condition=condition, body=body)
```

4. Add to `parse_pipeline_component()` in `commands.py`:
```python
elif self.parser.match(TokenType.MY_KEYWORD):
    return self.parse_my_structure_command()
```

5. Add executor method in `psh/executor/control_flow.py`

6. Add tests in `tests/unit/parser/`

### Adding a New Expression Type

1. Create or extend parser in `parsers/`

2. Add AST node in `psh/ast_nodes.py`

3. Wire into appropriate parsing method

4. Add visitor method in executor

## Key Implementation Details

### Keyword vs Word Distinction

Keywords are only recognized at command position:
- `if echo` → IF keyword, WORD "echo"
- `echo if` → WORD "echo", WORD "if"

The parser uses `TokenGroups.CONTROL_KEYWORDS` to identify keywords.

### Compound Command Handling

Compound commands can appear in pipelines:
```python
def parse_pipeline_component(self) -> Command:
    if self.parser.match(TokenType.WHILE):
        return self.parse_while_command()
    elif self.parser.match(TokenType.IF):
        return self.parse_if_command()
    # ... other compound commands
    else:
        return self.parse_command()  # Simple command
```

### Heredoc Collection

Heredocs are parsed in two phases:
1. Tokenization collects the `<<EOF` marker
2. Parser collects heredoc content after the command line

### Error Recovery

Multi-error mode collects errors instead of stopping:

```python
parser = Parser(tokens, config=ParserConfig(collect_errors=True))
result = parser.parse_with_error_collection()
# result.ast may be partial, result.errors contains all errors
```

## Testing

```bash
# Run parser unit tests
python -m pytest tests/unit/parser/ -v

# Test specific feature
python -m pytest tests/unit/parser/test_parser_migration.py -v

# Debug AST output
python -m psh --debug-ast -c "if true; then echo yes; fi"
```

## Common Pitfalls

1. **Token Advancement**: Always call `advance()` to consume tokens after matching. `match()` only peeks.

2. **Newline Handling**: Use `skip_newlines()` appropriately - some constructs allow them, others don't.

3. **Heredoc State**: Heredocs require special handling; they're collected after the statement.

4. **Context Preservation**: Use `with parser.context:` when entering nested parsing contexts.

5. **Error Position**: Always include token position in error messages for debugging.

## Debug Options

```bash
python -m psh --debug-ast      # Show parsed AST structure
python -m psh --debug-tokens   # Show tokens before parsing
python -m psh --validate       # Parse and validate without executing
```

## Word AST

The parser always builds **Word AST nodes** for command arguments. Each
`SimpleCommand.words` list contains `Word` objects with `LiteralPart` and
`ExpansionPart` nodes carrying per-part quote context (`quoted`, `quote_char`).

```python
# "hello $USER!" becomes:
Word(parts=[
    LiteralPart("hello ", quoted=True, quote_char='"'),
    ExpansionPart(VariableExpansion("USER"), quoted=True, quote_char='"'),
    LiteralPart("!", quoted=True, quote_char='"'),
], quote_type='"')
```

### WordBuilder

`WordBuilder` (`support/word_builder.py`) is the bridge between lexer
tokens and the Word AST. It is the most complex single piece of the
parser -- it handles RichToken decomposition, composite word merging,
and parameter expansion operator parsing.

**Entry point**: `CommandParser.parse_argument_as_word()` in
`parsers/commands.py`. This method detects composite sequences via
`TokenStream.peek_composite_sequence()`, then delegates to the
appropriate WordBuilder method.

**Three key operations**:

1. **Single tokens** -- `build_word_from_token()`: Decomposes
   double-quoted STRING tokens with `RichToken.parts` into
   `LiteralPart`/`ExpansionPart` nodes with per-part quote context.

2. **Composite tokens** -- `build_composite_word()`: Merges adjacent
   tokens (e.g. `"hello"$USER'!'`) into a single `Word` with per-part
   quote tracking.

3. **Expansion tokens** -- `parse_expansion_token()`: Parses VARIABLE,
   PARAM_EXPANSION, COMMAND_SUB, and ARITH_EXPANSION tokens into
   expansion AST nodes, handling operators like `${var:-default}`,
   `${var##pattern}`, etc.

## Configuration

`ParserConfig` controls parser behavior:

```python
@dataclass
class ParserConfig:
    strict_posix: bool = False        # POSIX-only syntax
    enable_bash_extensions: bool = True
    collect_errors: bool = False      # Multi-error mode
    max_errors: int = 10
```
